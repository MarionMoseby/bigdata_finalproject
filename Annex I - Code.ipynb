{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook shall serve as accompanying material to this repositories' README, a report for the “Big Data Engineering” subject at UPM’s Master in Computational Biology. It is thus only intended as a recopilation of used code; for the full discussion, please refer to the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.11\r\n"
     ]
    }
   ],
   "source": [
    "# First, we will create a Conda Environment to do all our processing\n",
    "# in python3.7, the only one orca supports\n",
    "\n",
    "#!conda create -n bigdataenv python=3.7\n",
    "#!source activate bigdataenv\n",
    "!python --version # We can check everything went successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1b71XxWRvZN",
    "outputId": "ee542e27-f375-4b72-f78f-5201ef901b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.13\" 2021-10-19\r\n",
      "OpenJDK Runtime Environment (build 11.0.13+8-Ubuntu-0ubuntu1.20.04)\r\n",
      "OpenJDK 64-Bit Server VM (build 11.0.13+8-Ubuntu-0ubuntu1.20.04, mixed mode, sharing)\r\n"
     ]
    }
   ],
   "source": [
    "# Now, we will install Java8\n",
    "\n",
    "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# Set environment variable JAVA_HOME.\n",
    "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "#!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "\n",
    "!java -version # And check it went correctly, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "id": "DBWLmbYcxcKn",
    "outputId": "fb87a702-b53c-4a9d-c89e-5467485f5168"
   },
   "outputs": [],
   "source": [
    "# And the latest pre-release version of BigDL Orca \n",
    "# Installing BigDL Orca from pip will automatically install pyspark, bigdl, and their dependencies.\n",
    "#!pip install --pre --upgrade bigdl-orca\n",
    "\n",
    "import findspark; findspark.init()\n",
    "from bigdl.orca import init_orca_context, stop_orca_context\n",
    "from bigdl.orca import OrcaContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VehCSB3lAf3",
    "outputId": "42abfa80-2df9-4d7b-d253-1a2944a2d0b6"
   },
   "outputs": [],
   "source": [
    "# Install python dependencies\n",
    "\n",
    "#!pip3 install torch==1.7.1 torchvision==0.8.2\n",
    "#!pip install six cloudpickle\n",
    "#!pip install jep==3.9.0\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VUTfSLztf7Uw"
   },
   "outputs": [],
   "source": [
    "# Define the path to the different folders:\n",
    "test_path = './chest_xray/test'\n",
    "train_path = './chest_xray/train'\n",
    "validation_path = './chest_xray/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A07A1wEvvKOH"
   },
   "source": [
    "We want to realize the following preprocessing: \n",
    "\n",
    "* CenterCrop - resizes the image to 224 x 224 \n",
    "* RandomFlip - Randomly flips 50% of the image horizontally \n",
    "* ColorJitter- Randomly adjust the brigthness of 50% of the images\n",
    "* Normalize - Normalize the images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kWnjM9btbMYE"
   },
   "outputs": [],
   "source": [
    "def custom_transform(sample):\n",
    "  transformer = torchvision.transforms.Compose([T.CenterCrop(size=(299, 299)), T.ToTensor(), \n",
    "                                                T.RandomHorizontalFlip(p=0.5),\n",
    "                                                T.ColorJitter(brightness=0.5, hue=0), \n",
    "                                                T.Normalize((0.5,), (0.5,)),])\n",
    "  return transformer(sample[\"image\"]), sample[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgImfD6FVZ9o"
   },
   "source": [
    "We create the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_Au8knKLWA6d"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#from skimage import io\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        normal_names = [\"NORMAL/\" + f for f in listdir(join(root_dir, \"NORMAL\")) if isfile(join(root_dir, \"NORMAL\", f))]\n",
    "        labels_normal = [0]*len(normal_names)\n",
    "        pneumonia_names = [\"PNEUMONIA/\" + f for f in listdir(join(root_dir, \"PNEUMONIA\")) if isfile(join(root_dir, \"PNEUMONIA\", f))]\n",
    "        labels_pneumonia = [1]*len(pneumonia_names)\n",
    "        self.labels = labels_normal\n",
    "        self.labels.extend(labels_pneumonia)\n",
    "        self.labels = np.asarray(self.labels, dtype=np.float32)\n",
    "        # labelling done\n",
    "\n",
    "        self.filenames = normal_names\n",
    "        self.filenames.extend(pneumonia_names)\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.filenames[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        #print(image.shape)\n",
    "        label = torch.Tensor([self.labels[idx]])\n",
    "        sample = {\"image\": image, \"label\": label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "TK6DTEDJZp5o",
    "outputId": "a88662d0-9309-48cc-b7ab-ed0a68386002"
   },
   "outputs": [],
   "source": [
    "train_data = CustomDataset(train_path, transform=custom_transform)\n",
    "val_data = CustomDataset(validation_path, transform=custom_transform)\n",
    "test_data = CustomDataset(test_path, transform=custom_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IHAlGciVQOk"
   },
   "source": [
    "We load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cAnyZp2hZQtb"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L-JTIqhoj3cQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images for training:  5216\n",
      "Number of images for testing:  624\n",
      "Number of images for validation:  16\n"
     ]
    }
   ],
   "source": [
    "# Check number of images \n",
    "print('Number of images for training: ', len(train_data))\n",
    "print('Number of images for testing: ', len(test_data))\n",
    "print('Number of images for validation: ', len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhKTv9rSVAM1"
   },
   "source": [
    "## 5. NEURAL NETWORK ESTRUCTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO_A7Wg-yxc9"
   },
   "source": [
    "### INTEGRATED STACKING NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-L-V90UVxjby"
   },
   "outputs": [],
   "source": [
    "# import necesary libraries and modules\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FT2xsXpdQ9LU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recommended to set it to True when running BigDL in Jupyter notebook. \n",
    "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
    "init_orca_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4Q7RETg5MtpR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32])\n",
      "torch.Size([8, 96])\n"
     ]
    }
   ],
   "source": [
    "# check that stacking works as expected\n",
    "a = torch.rand(size=(8, 32))\n",
    "b = torch.rand(size=(8, 32))\n",
    "c = torch.rand(size=(8, 32))\n",
    "print(a.shape)\n",
    "d = torch.cat((a, b, c), axis=-1)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSWWJQsnT5_r"
   },
   "source": [
    "At the end the stacking of the 5 neural networks because a problem of the memory ram (12 gb in google collab free) and the resources of the paper are bigger than the one we have. \n",
    "It is just not possible define the same neural network they did because the session crush due to the limit of the ram, but we define the code and check that the network works anyways: \n",
    "\n",
    "```\n",
    "# Define the network\n",
    "class IntegratedNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(IntegratedNet, self).__init__()\n",
    "\n",
    "    self.resnet18 = models.resnet18(pretrained=True)\n",
    "    self.resnet18.fc = nn.Linear(512, 32)\n",
    "\n",
    "    self.densenet = models.densenet161(pretrained=True)\n",
    "    self.densenet.classifier = nn.Linear(2208, 32)\n",
    "\n",
    "    self.inception = models.inception_v3(pretrained=True)\n",
    "    self.inception.fc = nn.Linear(2048, 32)\n",
    "    \n",
    "    self.mnasnet = models.mnasnet1_0(pretrained=True)\n",
    "    self.mnasnet.classifier = nn.Sequential(nn.Dropout(0.2, inplace=True),\n",
    "                                            nn.Linear(1280, 32))\n",
    "\n",
    "    self.mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "    self.mobilenet_v2.classifier = nn.Sequential(nn.Dropout(0.2), nn.Linear(1280, 32))\n",
    "    \n",
    "\n",
    "    self.fc_out = nn.Linear(2*32, 1)  # for binary classification, use single output\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_res = self.resnet18(x)\n",
    "    x_dense = self.densenet(x.detach())\n",
    "    x_inception = self.inception(x)[0]\n",
    "    x_mnas = self.mnasnet(x)\n",
    "    x_mobilenet = self.mobilenet_v2(x)\n",
    "    \n",
    "    #Concatenate the outputs\n",
    "    x = torch.cat((x_res, x_dense, x_inception, x_mnas, x_mobilenet), axis=-1)\n",
    "    x = self.fc_out(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "But with the actual resources we cannot use it, we define therefore another \n",
    "neural network stacking two models instead. Which is what google collab can handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fnFQIiyO2DZb"
   },
   "outputs": [],
   "source": [
    "# Define the network\n",
    "class IntegratedNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(IntegratedNet, self).__init__()\n",
    "\n",
    "    self.resnet18 = models.resnet18(pretrained=True)\n",
    "    self.resnet18.fc = nn.Linear(512, 32)\n",
    "\n",
    "    #self.densenet = models.densenet161(pretrained=True)\n",
    "    #self.densenet.classifier = nn.Linear(2208, 32)\n",
    "\n",
    "    #self.inception = models.inception_v3(pretrained=True)\n",
    "    #self.inception.fc = nn.Linear(2048, 32)\n",
    "    \n",
    "    self.mnasnet = models.mnasnet1_0(pretrained=True)\n",
    "    self.mnasnet.classifier = nn.Sequential(nn.Dropout(0.2, inplace=True),\n",
    "                                            nn.Linear(1280, 32))\n",
    "\n",
    "    #self.mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "    #self.mobilenet_v2.classifier = nn.Sequential(nn.Dropout(0.2), nn.Linear(1280, 32))\n",
    "    \n",
    "\n",
    "    self.fc_out = nn.Linear(2*32, 1)  # for binary classification, use single output\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_res = self.resnet18(x)\n",
    "    #x_dense = self.densenet(x.detach())\n",
    "    #x_inception = self.inception(x)[0]\n",
    "    x_mnas = self.mnasnet(x)\n",
    "    #x_mobilenet = self.mobilenet_v2(x)\n",
    "    #x_mobilenet = self.mobilenet_v2(x.detach())\n",
    "    \n",
    "    #Concatenate the outputs\n",
    "    #x = torch.cat((x_res, x_dense, x_inception, x_mnas, x_mobilenet), axis=-1)\n",
    "    x = torch.cat((x_res, x_mnas), axis=-1)\n",
    "    x = self.fc_out(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "byGhyoAmYgo9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/pablo/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5446c1a5da2c48a6a999b76eca0ee08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mnasnet1.0_top1_73.512-f206786ef8.pth\" to /home/pablo/.cache/torch/hub/checkpoints/mnasnet1.0_top1_73.512-f206786ef8.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69404a1f26541fb8b3d3dfa05262c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/16.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = IntegratedNet()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ovlS3kspYs0t"
   },
   "outputs": [],
   "source": [
    "# training loss vs. epochs\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "batch_size = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Y4DgOHbeJroc"
   },
   "outputs": [],
   "source": [
    "# we are still writing code# still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Eafs_Z6Skbuw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTorchLoss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.local/lib/python3.8/site-packages/bigdl/orca/torch/torch_model.py:82: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(new_weight) == 1, \"TorchModel's weights should be one tensor\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6666/2757120307.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccuracy\u001b[0m \u001b[0;31m# please dont strop we are still here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bigdl/orca/learn/pytorch/estimator.py\u001b[0m in \u001b[0;36mfrom_torch\u001b[0;34m(model, optimizer, loss, metrics, scheduler_creator, training_operator_cls, initialization_hook, config, scheduler_step_freq, use_tqdm, workers_per_node, model_dir, backend, sync_stats, log_level, log_to_driver)\u001b[0m\n\u001b[1;32m    102\u001b[0m                                          \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                          \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                                          bigdl_type=\"float\")\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spark\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch_pyspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyTorchPySparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bigdl/orca/learn/pytorch/pytorch_spark_estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, loss, optimizer, config, metrics, model_dir, bigdl_type)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mmodel_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bigdl/orca/torch/torch_loss.py\u001b[0m in \u001b[0;36mfrom_pytorch\u001b[0;34m(criterion)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mbys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTorchLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bigdl/orca/torch/torch_loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, criterion_bytes, bigdl_type)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mbigdl_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTorchLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigdl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bigdl/dllib/nn/criterion.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, jvalue, bigdl_type, *args)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigdl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         self.value = jvalue if jvalue else callBigDlFunc(\n\u001b[0;32m---> 39\u001b[0;31m             bigdl_type, JavaValue.jvm_class_constructor(self), *args)\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigdl_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigdl_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bigdl/dllib/utils/common.py\u001b[0m in \u001b[0;36mcallBigDlFunc\u001b[0;34m(bigdl_type, name, *args)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot find function: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mjinvoker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mJavaCreator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigdl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;31m# hasattr(jinvoker, name) always return true here,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;31m# so you need to invoke the method to check if it exist or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bigdl/dllib/utils/common.py\u001b[0m in \u001b[0;36minstance\u001b[0;34m(cls, bigdl_type, *args)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigdl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bigdl/dllib/utils/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bigdl_type, gateway)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mjclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreator_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbigdl_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"float\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ofFloat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mbigdl_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"double\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ofDouble\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from bigdl.orca.learn.pytorch import Estimator # don't stop, google\n",
    "from bigdl.orca.learn.metrics import Accuracy # please dont strop we are still here\n",
    "\n",
    "est = Estimator.from_torch(model=net, optimizer=optimizer, loss=criterion, metrics=[Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs-fGGn7qQq1"
   },
   "outputs": [],
   "source": [
    "from bigdl.orca.learn.trigger import EveryEpoch \n",
    "\n",
    "est.fit(data=train_loader, epochs=1, validation_data=test_loader,\n",
    "        checkpoint_trigger=EveryEpoch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gd2DejdHqUOx"
   },
   "outputs": [],
   "source": [
    "result = est.evaluate(data=test_loader)\n",
    "for r in result:\n",
    "    print(r, \":\", result[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYMbDnJmqWRK"
   },
   "outputs": [],
   "source": [
    "# stop orca context when program finishes\n",
    "stop_orca_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVLTncGkVGmi"
   },
   "source": [
    "## 6. VALIDATION OF THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVGErW_2pmWq"
   },
   "source": [
    "Here do the visualization of the loss, accuracy, visualize the model graph, the roc curve, the histograms and the confussion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q931J-RrVPX3"
   },
   "source": [
    "## 7. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfAjr0TWVUDS"
   },
   "source": [
    "## 8. REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xldlMPoWGHp"
   },
   "source": [
    "HERE PUT WITH CORRECT APA THE CITATIONS OF THE INTRODUCTION"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "project_big_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
